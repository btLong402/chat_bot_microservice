import os
from typing import List, Optional, Sequence

# Suppress noisy gRPC/ALTS logs (must be set before importing google libs)
os.environ.setdefault("GRPC_VERBOSITY", "ERROR")
os.environ.setdefault("GRPC_TRACE", "")

try:
    # Prefer the public Generative AI SDK. Avoid heavy setup at import time.
    import google.generativeai as _genai_pkg  # type: ignore
    _HAS_GENAI = True
except Exception:
    _genai_pkg = None
    _HAS_GENAI = False
try:
    from dotenv import load_dotenv
except Exception:
    # dotenv is optional; provide a no-op fallback so importing this module
    # doesn't fail in minimal environments.
    def load_dotenv():
        return None
from .memory import MemoryManager
from .retriever import RAGRetriever

load_dotenv()

import warnings

if not _HAS_GENAI:
    # don't raise at import time; let the application import the module.
    warnings.warn("google-genai package not installed. Install with: pip install google-genai", RuntimeWarning)

class GeminiBot:
    def __init__(
        self,
        name="La BÃ n AI",
        model="gemini-2.5-flash-lite",
        memory_file="chat_history.json",
        retriever=None,
        user_id: Optional[str] = None,
    ):
        self.name = name
        self.user_id = user_id
        self.memory = MemoryManager(memory_file)
        self.retriever = retriever or RAGRetriever()
        if not _HAS_GENAI or _genai_pkg is None:
            raise RuntimeError("google-generativeai package not installed. Install with: pip install google-generativeai")
        api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
        if api_key:
            try:
                _genai_pkg.configure(api_key=api_key)
            except Exception as exc:
                raise RuntimeError(f"google-generativeai configure failed: {exc}")
        else:
            warnings.warn("GEMINI_API_KEY not set. Set it via environment or .env file.", RuntimeWarning)
        try:
            self._genai_model = _genai_pkg.GenerativeModel(model)
        except Exception as exc:
            raise RuntimeError(f"Failed to initialize GenerativeModel '{model}': {exc}")
        # maintain legacy attributes for compatibility with older calling code
        self.model = None
        self.chat = None

    @property
    def generative_model(self):
        """Expose the underlying GenerativeModel for external orchestration."""
        return self._genai_model

    def ask(
        self,
        message,
        *,
        use_rag: bool = True,
        history_turns: int = 10,
        top_k: int = 3,
        temperature: Optional[float] = None,
        prompt_template: Optional[str] = None,
        return_details: bool = False,
        context_chunks: Optional[Sequence[str]] = None,
        user_id: Optional[str] = None,
    ):
        """
        Há»i Gemini vá»›i kháº£ nÄƒng ná»‘i máº¡ch há»™i thoáº¡i thá»±c (start_chat) vÃ  tá»± Ä‘á»™ng RAG.
        """

        active_user_id = user_id or self.user_id

        # ===== [1] RAG Context =====
        if context_chunks is not None:
            context_list = list(context_chunks)
        elif use_rag:
            context_list = self.retriever.retrieve(message, top_k=top_k)
        else:
            context_list = []
        context_block = "\n\n".join(context_list).strip() or "(khÃ´ng cÃ³ tÃ i liá»‡u tham chiáº¿u phÃ¹ há»£p)"

        # ===== [2] Lá»‹ch sá»­ há»™i thoáº¡i =====
        conversation = self.memory.get_conversation(user_id=active_user_id) or []
        history_for_prompt = []
        for role, content in conversation[-history_turns:]:
            label = "NgÆ°á»i dÃ¹ng" if role == "user" else self.name
            history_for_prompt.append(f"{label}: {content}")
        history_block = "\n".join(history_for_prompt).strip() or "(khÃ´ng cÃ³ lá»‹ch sá»­ phÃ¹ há»£p)"

        # ===== [3] Náº¿u cÃ³ prompt template tÃ¹y chá»‰nh =====
        if prompt_template:
            try:
                prompt = prompt_template.format(
                    assistant_name=self.name,
                    query=message,
                    history_block=history_block,
                    context_block=context_block,
                    history=history_block,
                    context=context_block,
                )
            except Exception as exc:
                raise ValueError(f"Invalid prompt_template: {exc}")
        else:
            # Prompt chuáº©n (máº·c Ä‘á»‹nh)
           prompt = f""" [1] Vai trÃ² (Role): Báº¡n lÃ  **Du há»c Mentor Pro** (tÃªn: {self.name}) â€” Trá»£ lÃ½ AI tÆ° váº¥n du há»c chuyÃªn nghiá»‡p cho há»c sinh â€“ sinh viÃªn Viá»‡t Nam. Hiá»ƒu rÃµ vá» ngÃ nh, há»c bá»•ng, visa, chi phÃ­ vÃ  Ä‘iá»u kiá»‡n Ä‘áº§u vÃ o táº¡i cÃ¡c nÆ°á»›c phá»• biáº¿n (Má»¹, Canada, Ãšc, Anh, Nháº­t, HÃ n, Singapore...). **Phong cÃ¡ch:** thá»±c táº¿, logic, khÃ´ng suy diá»…n cáº£m xÃºc, chá»‰ pháº£n há»“i dá»±a trÃªn dá»¯ liá»‡u cÃ³ trong input. [2] Má»¥c tiÃªu (Goal): Táº¡o pháº£n há»“i ngáº¯n gá»n, Ä‘á»‹nh hÆ°á»›ng vÃ  hÃ nh Ä‘á»™ng rÃµ â€” giÃºp ngÆ°á»i dÃ¹ng xÃ¡c Ä‘á»‹nh hÆ°á»›ng Ä‘i du há»c phÃ¹ há»£p nháº¥t. [3A] Ngá»¯ cáº£nh dá»¯ liá»‡u (Context Data): - Tin nháº¯n: {message} - TÃ i liá»‡u tham chiáº¿u (context_block): {context_block} **CÃ¡ch dÃ¹ng context_block:** - DÃ¹ng *chá»‰ khi* ná»™i dung ngÆ°á»i dÃ¹ng chá»©a â‰¥2 tá»« khÃ³a trÃ¹ng vá»›i context (vÃ­ dá»¥: tÃªn trÆ°á»ng, ngÃ nh, nÄƒm). - Náº¿u message khÃ´ng liÃªn quan (chÃ o há»i / ngoÃ i chá»§ Ä‘á» du há»c) â†’ bá» qua context_block. - Náº¿u cÃ³ mÃ¢u thuáº«n giá»¯a message vÃ  context_block â†’ Æ°u tiÃªn message hiá»‡n táº¡i vÃ  ghi chÃº: â€œLÆ°u Ã½: thÃ´ng tin trong context khÃ¡c vá»›i yÃªu cáº§u má»›i â€” dÃ¹ng dá»¯ liá»‡u má»›i Ä‘á»ƒ phÃ¢n tÃ­ch.â€ [3B] Ngá»¯ cáº£nh há»™i thoáº¡i (Conversation History): {history_block} **CÃ¡ch dÃ¹ng history_block:** - Náº¿u trá»‘ng hoáº·c khÃ´ng liÃªn quan â†’ xem lÃ  cuá»™c trÃ² chuyá»‡n má»›i. - Náº¿u Ä‘Ã£ cÃ³ há»™i thoáº¡i â†’ khÃ´ng chÃ o láº¡i, tráº£ lá»i tiáº¿p máº¡ch láº¡c. - Náº¿u chá»©a quyáº¿t Ä‘á»‹nh trÆ°á»›c (quá»‘c gia/ngÃ nh Ä‘Ã£ chá»n) â†’ tÃ³m táº¯t ngáº¯n trÆ°á»›c khi Ä‘á» xuáº¥t, khÃ´ng há»i láº¡i. - Náº¿u mÃ¢u thuáº«n â†’ Æ°u tiÃªn yÃªu cáº§u má»›i vÃ  bÃ¡o rÃµ. [4] Quy táº¯c & Phong cÃ¡ch (Rules & Style): - NgÃ´n ngá»¯: tiáº¿ng Viá»‡t thÃ¢n thiá»‡n, rÃµ rÃ ng, Ä‘Ãºng ngá»¯ phÃ¡p. - Pháº£n há»“i theo máº¡ch tá»± nhiÃªn, khÃ´ng Ã©p cáº¥u trÃºc, nhÆ°ng váº«n cÃ³ trÃ¬nh tá»± há»£p lÃ½: (1) Náº¯m báº¯t vÃ  xÃ¡c nháº­n yÃªu cáº§u. (2) PhÃ¢n tÃ­ch 2â€“3 Ä‘iá»ƒm trá»ng tÃ¢m (Ä‘iá»u kiá»‡n, cÆ¡ há»™i, lÆ°u Ã½). (3) ÄÆ°a hÆ°á»›ng hÃ nh Ä‘á»™ng hoáº·c cÃ¢u há»i tiáº¿p theo. - TrÃ¡nh liá»‡t kÃª khÃ´ cá»©ng; Æ°u tiÃªn diá»…n Ä‘áº¡t nhÆ° mentor hÆ°á»›ng dáº«n thá»±c táº¿. ğŸ“**Náº¿u thiáº¿u dá»¯ liá»‡u:** - Há»i tá»‘i Ä‘a 3 cÃ¢u ngáº¯n, theo **thá»© tá»± Æ°u tiÃªn**: (1) Báº­c há»c â†’ (2) NgÃ nh â†’ (3) Quá»‘c gia â†’ (4) NgÃ¢n sÃ¡ch â†’ (5) Háº¡n/visa. - Khi user Ä‘Ã£ cho 1 má»¥c, bá» qua cÃ¢u Ä‘Ã³, há»i tiáº¿p má»¥c káº¿ tiáº¿p. - Máº«u: "Äá»ƒ gá»£i Ã½ nhanh: 1) Báº­c há»c? 2) NgÃ nh? 3) NgÃ¢n sÃ¡ch (VND/nÄƒm)?" ğŸ“**Náº¿u user há»i ngoÃ i pháº¡m vi du há»c:** - Tráº£ fallback: "Hiá»‡n tÃ´i chuyÃªn tÆ° váº¥n du há»c. Vá»›i cÃ¢u há»i nÃ y, tÃ´i cÃ³ thá»ƒ: (A) chuyá»ƒn sang chá»§ Ä‘á» liÃªn quan du há»c náº¿u cÃ³, hoáº·c (B) gá»£i Ã½ nguá»“n/tá»« khÃ³a Ä‘á»ƒ báº¡n tra cá»©u â€” báº¡n chá»n A hay B?" - Náº¿u chá»§ Ä‘á» phÃ¡p lÃ½ / y táº¿ / tÃ i chÃ­nh chuyÃªn sÃ¢u â†’ cáº£nh bÃ¡o: "TÃ´i khÃ´ng pháº£i luáº­t sÆ°/bÃ¡c sÄ©; báº¡n nÃªn tham kháº£o chuyÃªn gia chÃ­nh thá»©c." ğŸ“**Tone chÃ o Ä‘áº§u (náº¿u lÃ  tin nháº¯n Ä‘áº§u tiÃªn):** Chá»n ngáº«u nhiÃªn 1 trong 4 biáº¿n thá»ƒ: 1. "ChÃ o (name)! Báº¡n Ä‘ang cÃ¢n nháº¯c du há»c báº­c nÃ o?" 2. "Xin chÃ o â€” nÃ³i tÃ´i nghe 2 Ä‘iá»u báº¡n quan tÃ¢m nháº¥t vá» du há»c nhÃ© (quá»‘c gia & ngÃ nh)?" 3. "ChÃ o báº¡n, tÃ´i cÃ³ thá»ƒ giÃºp chá»n trÆ°á»ng hoáº·c check há»c bá»•ng â€” báº¡n muá»‘n gÃ¬ trÆ°á»›c?" 4. "Ráº¥t vui Ä‘Æ°á»£c giÃºp! Báº¡n Ä‘Ã£ cÃ³ quá»‘c gia hoáº·c ngÃ nh há»c trong Ä‘áº§u chÆ°a?" ğŸ“**Edge cases:** - User chá»‰ chÃ o / small talk â†’ Ä‘Ã¡p 1 cÃ¢u + CTA: "Báº¡n quan tÃ¢m Ä‘iá»u gÃ¬ vá» du há»c?" - User gá»­i file CV / transcript â†’ tÃ³m táº¯t 3 Ä‘iá»ƒm máº¡nh rá»“i há»i 2 cÃ¢u Æ°u tiÃªn. - User há»i tÃ­nh chi phÃ­ â†’ Ä‘Æ°a khoáº£ng giÃ¡ + giáº£ Ä‘á»‹nh + ghi chÃº nguá»“n. - User xin danh sÃ¡ch trÆ°á»ng â†’ top 5, cÃ³ lÃ½ do + tiÃªu chÃ­ rÃµ rÃ ng. ğŸ“**Äá»™ tin cáº­y (Confidence Tag):** - Cao (â‰¥80%): nguá»“n chÃ­nh phá»§ / Ä‘áº¡i há»c / tá»• chá»©c há»c bá»•ng chÃ­nh thá»©c. - Trung bÃ¬nh (50â€“80%): nguá»“n blog hoáº·c diá»…n Ä‘Ã n cÃ³ kiá»ƒm chá»©ng. - Tháº¥p (<50%): suy luáº­n / Æ°á»›c lÆ°á»£ng, pháº£i ghi rÃµ giáº£ Ä‘á»‹nh. â†’ Khi nÃªu sá»‘ liá»‡u, thÃªm vÃ­ dá»¥: â€œTin cáº­y: 85% â€” theo website chÃ­nh phá»§ Canada, cáº­p nháº­t 03/2025.â€ [5] Äá»‹nh dáº¡ng Ä‘áº§u ra (Output Format): - Náº¿u **tin nháº¯n Ä‘áº§u tiÃªn** (history_block trá»‘ng) â†’ chÃ o ngáº¯n gá»n (1 trong 4 máº«u trÃªn). - Náº¿u **Ä‘Ã£ cÃ³ há»™i thoáº¡i** â†’ khÃ´ng chÃ o láº¡i, tráº£ lá»i trá»±c tiáº¿p. - Náº¿u **cÃ³ Ä‘á»§ dá»¯ kiá»‡n** â†’ pháº£n há»“i theo 4 pháº§n chuáº©n. - Náº¿u **thiáº¿u dá»¯ kiá»‡n** â†’ kÃ­ch hoáº¡t cháº¿ Ä‘á»™ há»i ngáº¯n (â‰¤4 cÃ¢u). - Náº¿u **ngoÃ i pháº¡m vi du há»c** â†’ dÃ¹ng fallback Ä‘á»‹nh dáº¡ng chuáº©n. [6] Kiá»ƒm chá»©ng & Äá»‘i chiáº¿u (Validation): - So sÃ¡nh dá»¯ liá»‡u giá»¯a {message}, {context_block}, {history_block}. - Æ¯u tiÃªn dá»¯ liá»‡u liÃªn káº¿t, bá» dá»¯ liá»‡u mÃ¢u thuáº«n. - Náº¿u chÆ°a Ä‘á»§ dá»¯ liá»‡u â†’ nÃ³i rÃµ â€œchÆ°a Ä‘á»§ dá»¯ kiá»‡n Ä‘á»ƒ káº¿t luáº­nâ€. - Vá»›i sá»‘ liá»‡u cá»¥ thá»ƒ (chi phÃ­, Ä‘iá»ƒm, deadline) â†’ ghi rÃµ pháº¡m vi + nguá»“n. - BÃ¡o â€œTin cáº­y: XX% â€” theo nguá»“n Y (thÃ¡ng/nÄƒm)â€ hoáº·c â€œÆ¯á»›c lÆ°á»£ng náº¿u khÃ´ng cÃ³ nguá»“nâ€. """

        # ===== [4] Sinh pháº£n há»“i vá»›i cÆ¡ cháº¿ ná»‘i máº¡ch =====
        generation_kwargs = {"generation_config": {"temperature": temperature}} if temperature else {}

        try:
            if conversation:
                # CÃ³ lá»‹ch sá»­ há»™i thoáº¡i â†’ dÃ¹ng start_chat()
                if not self.chat:
                    # Táº¡o chat session tá»« lá»‹ch sá»­
                    formatted_history = [
                        {"role": "user" if r == "user" else "model", "parts": c}
                        for r, c in conversation[-history_turns:]
                    ]
                    self.chat = self._genai_model.start_chat(history=formatted_history)
                # Gá»­i tin nháº¯n má»›i ná»‘i máº¡ch
                response = self.chat.send_message(message, **generation_kwargs)
                answer = response.text.strip()
            else:
                # KhÃ´ng cÃ³ lá»‹ch sá»­ â†’ láº§n Ä‘áº§u (dÃ¹ng generate_content)
                response = self._genai_model.generate_content(prompt, **generation_kwargs)
                answer = getattr(response, "text", str(response)).strip()

        except Exception as e:
            raise RuntimeError(f"Gemini request failed: {e}")

        # ===== [5] Cáº­p nháº­t bá»™ nhá»› =====
        self.memory.add_message("user", message, user_id=active_user_id)
        self.memory.add_message("assistant", answer, user_id=active_user_id)

        # ===== [6] Tráº£ káº¿t quáº£ =====
        if return_details:
            return {
                "answer": answer,
                "context": context_list,
                "prompt": prompt,
                "history_block": history_block,
            }
        return answer

    def clear_context(self):
        self.memory.clear_history(user_id=self.user_id)
        # No persistent chat object when using the `google-genai` client
        # in this simplified adapter; just clear conversation memory.
        self.chat = None
        return "ğŸ§¹ Bá»™ nhá»› há»™i thoáº¡i Ä‘Ã£ Ä‘Æ°á»£c xÃ³a!"

